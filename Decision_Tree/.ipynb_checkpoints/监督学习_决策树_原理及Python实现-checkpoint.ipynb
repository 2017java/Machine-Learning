{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-信息熵-Information-Entropy\" data-toc-modified-id=\"1.-信息熵-Information-Entropy-1\">1. 信息熵 Information Entropy</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-信息熵公式推导\" data-toc-modified-id=\"1.1-信息熵公式推导-1.1\">1.1 信息熵公式推导</a></span></li></ul></li><li><span><a href=\"#2.-信息增益-Information-Gain\" data-toc-modified-id=\"2.-信息增益-Information-Gain-2\">2. 信息增益 Information Gain</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1-信息增益最大化\" data-toc-modified-id=\"2.1-信息增益最大化-2.1\">2.1 信息增益最大化</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1.1-利用离散特征进行分类\" data-toc-modified-id=\"2.1.1-利用离散特征进行分类-2.1.1\">2.1.1 利用离散特征进行分类</a></span></li><li><span><a href=\"#2.1.2-利用连续特征进行分类\" data-toc-modified-id=\"2.1.2-利用连续特征进行分类-2.1.2\">2.1.2 利用连续特征进行分类</a></span></li></ul></li></ul></li><li><span><a href=\"#3.-Random-Forests-随机森林\" data-toc-modified-id=\"3.-Random-Forests-随机森林-3\">3. Random Forests 随机森林</a></span></li><li><span><a href=\"#4.-Hyperparameter-of-Decision-Tree-决策树的超参数\" data-toc-modified-id=\"4.-Hyperparameter-of-Decision-Tree-决策树的超参数-4\">4. Hyperparameter of Decision Tree 决策树的超参数</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1-Maximun-depth-最大深度\" data-toc-modified-id=\"4.1-Maximun-depth-最大深度-4.1\">4.1 Maximun depth 最大深度</a></span></li><li><span><a href=\"#4.2-Minimum-samples-per-leaf-每片叶子的最小样本数\" data-toc-modified-id=\"4.2-Minimum-samples-per-leaf-每片叶子的最小样本数-4.2\">4.2 Minimum samples per leaf 每片叶子的最小样本数</a></span></li><li><span><a href=\"#4.3-每次分裂的最小样本数\" data-toc-modified-id=\"4.3-每次分裂的最小样本数-4.3\">4.3 每次分裂的最小样本数</a></span></li><li><span><a href=\"#4.4-最大特征数\" data-toc-modified-id=\"4.4-最大特征数-4.4\">4.4 最大特征数</a></span></li></ul></li><li><span><a href=\"#5.-Decision-Tree-in-Sklearn-Sklearn-中的决策树\" data-toc-modified-id=\"5.-Decision-Tree-in-Sklearn-Sklearn-中的决策树-5\">5. Decision Tree in Sklearn Sklearn 中的决策树</a></span><ul class=\"toc-item\"><li><span><a href=\"#5.1-Hyperparameter-超参数\" data-toc-modified-id=\"5.1-Hyperparameter-超参数-5.1\">5.1 Hyperparameter 超参数</a></span></li><li><span><a href=\"#5.2-Example-实例\" data-toc-modified-id=\"5.2-Example-实例-5.2\">5.2 Example 实例</a></span></li></ul></li><li><span><a href=\"#参考资料\" data-toc-modified-id=\"参考资料-6\">参考资料</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 信息熵 Information Entropy \n",
    "\n",
    "\n",
    "信息是个很抽象的概念。人们常常说信息很多，或者信息较少，但却很难说清楚信息到底有多少。比如一本五十万字的中文书到底有多少信息量。\n",
    "直到1948年，香农提出了“信息熵”的概念，才解决了对信息的量化度量问题。\n",
    "\n",
    "`信息熵`这个词是香农从热力学中借用过来的。热力学中的热熵是表示分子状态混乱程度的物理量。香农用信息熵的概念来`描述不确定性`。\n",
    "\n",
    "* 随机变量$x$的信息熵计算公式：$H(x)= -\\sum_{i=1}^nP(x_i)log_2(P_i)$\n",
    "* 信息熵越大，则表示不确定性越高。\n",
    "* 在文本中，当不同的词汇越多时，其信息熵越大，直观上来说就是所包含的信息越多\n",
    "\n",
    ">熵：在物理的热力学中，用熵来表示分子状态混乱程度。当一个物体温度越高时，其内部粒子活动越剧烈，也越混乱。因此混乱程度越高，熵越大。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 信息熵公式推导\n",
    "\n",
    "\n",
    "1. 计算取出结果与原顺序相同的概率：\n",
    "\n",
    "$$P(x_i)=P(x_1)\\times P(x_2) \\times...\\times P(x_n)\\tag{1}$$\n",
    "\n",
    "2. 将概率公式取以2为底的对数变换，得到信息量$I(x_i)$的公式：\n",
    "\n",
    "$$I(x_i)=log_2(\\frac{1}{P(x_i)})=-log_2(P(x_i))\\tag{2}$$\n",
    "\n",
    "3. 随机变量$x$的信息熵计算公式：: \n",
    "\n",
    "$$H(x)=E[I(x_i)]=-E[log_2(P(x_i))]= -\\sum_{i=1}^nP(x_i)log_2(P_i)\\tag{3}$$\n",
    "\n",
    "对于样本集合$D$来说，随机变量$x$是样本的类别，即假设样本有$k$个类别，样本总数为$D$，则类别$i$的概率是$\\frac{c_i}{D}$。\n",
    "\n",
    "因此样本集合$D$的经验熵为：\n",
    "$$H(D)=-\\sum_{i=1}^k \\frac{c_i}{D}log_2(\\frac{c_i}{D}) \\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举例：假设有四个球，从中随机放回地抽出四个球，下面计算各个事件的信息熵：\n",
    "\n",
    "1.`红红红红`，则事件$x$的信息熵：\n",
    "\n",
    "$H(x)=-\\sum_{i=1}^4P(x_i)log_2(P_i)\n",
    "=[1 \\times log_2(1)+1 \\times log_2(1)+1 \\times log_2(1)+1 \\times log_2(1)]=0$\n",
    "\n",
    "2.`红红红蓝`，则事件$y$的信息熵：\n",
    "\n",
    "$H(y)=-\\sum_{i=1}^4P(y_i)log_2(P_i)\n",
    "=[0.75 \\times log_2(0.75)+0.75 \\times log_2(0.75)+0.75 \\times log_2(0.75)+0.25 \\times log_2(0.25)]=1.4338$\n",
    "\n",
    "3.`红红蓝蓝`，则事件$z$的信息熵：\n",
    "\n",
    "$H(z)=-\\sum_{i=1}^4P(z_i)log_2(P_i)\n",
    "=[0.5 \\times log_2(0.5)+0.5 \\times log_2(0.5)+0.5 \\times log_2(0.5)+0.5 \\times log_2(0.5)]=2$\n",
    "\n",
    "由上面三个例子可以看出，当混乱程度越高时，信息熵越大。\n",
    "\n",
    "关于各类熵的定义及推导，可参考[这篇文章](https://blog.csdn.net/Tomcater321/article/details/80699044)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 信息增益 Information Gain\n",
    "\n",
    "**信息增益：**\n",
    "\n",
    "对于待划分的数据集$D$，其信息熵大小是固定的，但是划分后子代熵之和是不定的。子代熵越小说明使用此特征划分得到的子集不确定性越小（纯度越高），因此`父代与子代的信息熵之差`（`信息增益`）越大，说明使用当前特征划分数据集$D$时，其纯度上升的更快<sup>[1]。\n",
    "\n",
    "\n",
    "**与决策树关系：**\n",
    "\n",
    "我们在构建最优的决策树时总希望能够快速到达村度更高的集合，这一点可以参考优化算法中的梯度下降（每一步沿着负梯度方法最小化损失函数使函数值快速减小）同理，在决策树构建的过程中我们总是希望集合往最快到达纯度更高的子集合方向发展，因此我们总是选择使得信息增益最大的特征来划分当前数据集%D%。\n",
    "\n",
    "使用特征$A$划分数据集$D$，计算划分前后各个数据集的信息熵，并计算`信息增益`：\n",
    "\n",
    "$$g(D,A)=H(D)-H(D|A) \\tag{5}$$\n",
    "\n",
    "其中，假设特征$A$将数据集$D$划分为$n$个子代，则$H(D|A)$为划分后子代的平均信息熵：\n",
    "\n",
    "$$H(D|A)= \\sum_{i=1}^nP(A_i)H(A_i) \\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举例：假设我们在调查性别与活跃度哪一个对用户流失影响越大<sup>[2]，样本如下：\n",
    "\n",
    "Gender | Activation | is_lost  \n",
    "-|-|-\n",
    "M | H | 0 |\n",
    "F | M | 0 |\n",
    "M | L | 1 |\n",
    "F | H | 0 |\n",
    "M | H | 0 |\n",
    "M | M | 0 |\n",
    "M | M | 1 |\n",
    "F | M | 0 |\n",
    "F | L | 1 |\n",
    "F | M | 0 |\n",
    "F | H | 0 |\n",
    "M | L | 1 |\n",
    "F | L | 0 |\n",
    "M | H | 0 |\n",
    "M | H | 0 |\n",
    "\n",
    "**按性别分类：**\n",
    "\n",
    " \\ | Lost | No-lost | Sum\n",
    "-|-|-|-\n",
    " Whole | 5 | 10 | 15\n",
    " Male | 3 | 5 | 8 \n",
    " Female | 2 | 5 | 7 \n",
    "\n",
    "整体熵：\n",
    "\n",
    "$E(S)=-[\\frac{5}{15}log_2(\\frac{5}{15})+\\frac{10}{15}log_2(\\frac{10}{15})]=0.9182$\n",
    "\n",
    "性别熵：\n",
    "\n",
    "$E(g_1)=-[\\frac{3}{8}log_2(\\frac{3}{8})+\\frac{5}{8}log_2(\\frac{5}{8})]=0.0.9543$\n",
    "\n",
    "$E(g_2)=-[\\frac{2}{7}log_2(\\frac{2}{7})+\\frac{2}{7}log_2(\\frac{2}{7})]=0.8631$\n",
    "\n",
    "`性别信息增益`：\n",
    "$g(S|g)=E(S)-\\frac{8}{15}E(g_1)-\\frac{7}{15}E(g_2)=0.0064$\n",
    "\n",
    "\n",
    "**按活跃度分类**\n",
    "\n",
    " \\ | Lost | No-lost | Sum\n",
    "-|-|-|-\n",
    " Whole | 5 | 10 | 15\n",
    " Hight | 0 | 6 | 6 \n",
    " Mid | 1 | 4 | 5 \n",
    " Low | 4 | 0 | 4 \n",
    " \n",
    "活跃度熵：\n",
    "\n",
    "$E(a_1)=0$\n",
    "\n",
    "$E(a_2)=0.7219$\n",
    "\n",
    "$E(a_2)=0$\n",
    "\n",
    "`活跃度信息增益`：\n",
    "$g(S|a)=E(S)-\\frac{6}{15}E(a_1)-\\frac{5}{15}E(a_2)-\\frac{4}{15}E(a_3)=0.0064$\n",
    "\n",
    "活跃度信息增益比性别信息增益大，也就是说，活跃度对用户流失的影响比性别大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 信息增益最大化\n",
    "\n",
    "### 2.1.1 利用离散特征进行分类\n",
    "\n",
    "1. 计算父代熵\n",
    "2. 计算子代熵\n",
    "3. 计算信息增益，按信息增益较大的进行分类\n",
    "4. 如果分类后结果不理想，可将其分类作为父代继续进行分类\n",
    "\n",
    "<img style=\"float:left\" src=\"https://x1a-alioss.oss-cn-shenzhen.aliyuncs.com/0E11B805-6B09-421D-AE03-729886739EEF.png\" width=\"520\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 利用连续特征进行分类\n",
    "\n",
    "1. 水平切法\n",
    "2. 垂直分法\n",
    "3. 选择熵较大的分法\n",
    "4. 建立决策树反复迭代\n",
    "\n",
    "<img style=\"float:left\" src=\"https://x1a-alioss.oss-cn-shenzhen.aliyuncs.com/6DE15C85-7F86-4A31-9CA2-933DACD14F11.png\" width=\"520\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forests 随机森林\n",
    "\n",
    "* 目的：防止决策树过拟合\n",
    "\n",
    "* 思想：随机选择几个特征生成一个决策树并作出预测，反复生成多个决策树后取预测结果最多的作为预测值\n",
    "\n",
    "<img style=\"float:left\" src=\"https://x1a-alioss.oss-cn-shenzhen.aliyuncs.com/B2636383-E615-47A6-B755-9893421FF4C9.png\" width=\"520\" >\n",
    "\n",
    "<img style=\"float:left\" src=\"https://x1a-alioss.oss-cn-shenzhen.aliyuncs.com/91F4129F-EA27-4C34-9EC2-8E2B14B23167.png\" width=\"520\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hyperparameter of Decision Tree 决策树的超参数\n",
    "\n",
    "## 4.1 Maximun depth 最大深度\n",
    "\n",
    "* 决策树的最大深度指树根和叶子之间的最大长度。当决策树的最大深度为$k$时，它最多可以拥有$2^k$片叶子。\n",
    "\n",
    "* 较大的深度往往会导致过拟合，这是因为过深的决策树可以记忆数据。而较小的深度会使得模型过于简单，导致欠拟合。\n",
    "![942fc8371812cfba2633a679eed21bff.png]\n",
    "\n",
    "<img style=\"float:left\" src=\"https://x1a-alioss.oss-cn-shenzhen.aliyuncs.com/1140D5F7-A786-4302-9D48-7A7A7A878A93.png\" width=\"520\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Minimum samples per leaf 每片叶子的最小样本数\n",
    "\n",
    "* 在分裂节点时，很有可能一片叶子上有 99 个样本，而另一片叶子上只有 1 个样本。这将使我们陷入困境，并造成资源和时间的浪费。如果想避免这种问题，我们可以设置每片叶子允许的最小样本数。\n",
    "\n",
    "* 这个数字可以被指定为一个整数，也可以是一个浮点数。如果它是整数，它将表示这片叶子上的最小样本数。如果它是个浮点数，它将被视作每片叶子上的最小样本比例。比如，0.1 或 10% 表示如果一片叶子上的样本数量小于该节点中样本数量的 10%，这种分裂将不被允许。\n",
    "\n",
    "* 当每片叶子的样本数量较小时，叶子上的样本数量也有可能过于稀少，此时模型将记忆数据，也就是过拟合。当每片叶子的样本数量较大时，决策树能够获得足够的弹性进行构建，这也许会导致欠拟合。\n",
    "\n",
    "<img style=\"float:left\" src=\"https://x1a-alioss.oss-cn-shenzhen.aliyuncs.com/839F227C-3645-49C5-B23F-ADBD3C505381.png\" width=\"520\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 每次分裂的最小样本数\n",
    "* 与每片叶子上的最小样本树相同，只不过是应用在节点的分裂当中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 最大特征数\n",
    "* 有时，我们会遇到特征数量过于庞大，而无法建立决策树的情况。在这种状况下，对于每一个分裂，我们都需要检查整个数据集中的每一个特征。这种过程极为繁琐。而解决方案之一是限制每个分裂中查找的特征数。如果这个数字足够庞大，我们很有可能在查找的特征中找到良好特征（尽管也许并不是完美特征）。然而，如果这个数字小于特征数，这将极大加快我们的计算速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Decision Tree in Sklearn Sklearn 中的决策树\n",
    "``` Python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = DecisionTreeClassifier()\n",
    "model = DecisionTreeClassifier(max_depth = 7, min_samples_leaf = 10)\n",
    "model.fit(x_values, y_values)\n",
    "y_pred = model.predict(X)\n",
    "acc = accuracy_score(y,y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Hyperparameter 超参数\n",
    "* max_depth：树中的最大层级数量。（预剪枝）\n",
    "* min_samples_leaf：叶子允许的最低样本数量。\n",
    "* min_samples_split：拆分内部节点所需的最低样本数量。\n",
    "* max_features：寻找最佳拆分方法时要考虑的特征数量。\n",
    "``` Python\n",
    "model = DecisionTreeClassifier(max_depth = 7, min_samples_leaf = 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5.2 Example 实例\n",
    "``` Python\n",
    "# Import statements \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the data.\n",
    "data = np.asarray(pd.read_csv('data.csv', header=None))\n",
    "# Assign the features to the variable X, and the labels to the variable y. \n",
    "X = data[:,0:2]\n",
    "y = data[:,2]\n",
    "\n",
    "# TODO: Create the decision tree model and assign it to the variable model.\n",
    "# You won't need to, but if you'd like, play with hyperparameters such\n",
    "# as max_depth and min_samples_leaf and see what they do to the decision\n",
    "# boundary.\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# TODO: Fit the model.\n",
    "model.fit(X, y)\n",
    "# TODO: Make predictions. Store them in the variable y_pred.\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# TODO: Calculate the accuracy and assign it to the variable acc.\n",
    "acc = accuracy_score(y,y_pred)\n",
    "\n",
    "# 可视化\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(model,out_file=\"tree.dot\",class_names=['a','b'],impurity=False,filled=True)\n",
    "# 通过terminal cd到文件夹\n",
    "# 运行 dot -Tpdf Tree.dot -o output.pdf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考资料\n",
    "\n",
    "[1] Tomcater321.决策树--信息增益，信息增益比，Geni指数的理解\n",
    "[EB/OL].https://blog.csdn.net/Tomcater321/article/details/80699044, 2018-06-14.\n",
    "\n",
    "[2] It_BeeCoder.机器学习中信息增益的计算方法 [EB/OL].https://blog.csdn.net/it_beecoder/article/details/79554388, 2018-03-14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
