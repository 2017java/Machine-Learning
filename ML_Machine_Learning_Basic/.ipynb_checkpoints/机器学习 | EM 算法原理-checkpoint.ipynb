{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#EM-算法\" data-toc-modified-id=\"EM-算法-1\">EM 算法</a></span></li><li><span><a href=\"#1.-EM-算法的引入\" data-toc-modified-id=\"1.-EM-算法的引入-2\">1. EM 算法的引入</a></span><ul class=\"toc-item\"><li><span><a href=\"#三硬币模型\" data-toc-modified-id=\"三硬币模型-2.1\">三硬币模型</a></span></li></ul></li><li><span><a href=\"#2.-EM-算法\" data-toc-modified-id=\"2.-EM-算法-3\">2. EM 算法</a></span><ul class=\"toc-item\"><li><span><a href=\"#Q-函数\" data-toc-modified-id=\"Q-函数-3.1\">Q 函数</a></span></li></ul></li><li><span><a href=\"#参考文献\" data-toc-modified-id=\"参考文献-4\">参考文献</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相关文章：\n",
    "\n",
    "[机器学习 | 目录](https://blog.csdn.net/weixin_45488228/article/details/99691709)\n",
    "\n",
    "本文大部分内容搬运自李航老师的《统计学习方法》<sup>[1]</sup>，以给出 EM 算法较为完整的定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM 算法\n",
    "\n",
    "`EM 算法`是一种迭代算法，1977 年由 Dempster 等人总结提出，用于含有`隐变量`（hidden variable）的概率模型参数的`极大似然估计`，或极大后验估计。\n",
    "\n",
    "EM 算法的每次迭代由两步组成：**E 步**：求期望（expectation）；**M 步**：求极大（maximization）。所以这一算法称为`期望极大算法`（expectation maximization algorithm, EM）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. EM 算法的引入\n",
    "\n",
    "概率模型有时即含有`观测数据`（observable varible，已知），又含有`隐变量`或`潜在变量`（latent varible，未知），如果概率模型的变量都是观测变量，那么给定数据，可以直接使用极大似然估计，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计方法。**EM 算法就是含有隐变量的概率模型参数的极大似然估计**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三硬币模型\n",
    "\n",
    "假设有三枚硬币，分别记作 A，B，C。这些硬币正面出现的概率分别是 $\\pi$，$p$ 和 $q$。进行如下掷硬币试验：\n",
    "\n",
    "先掷硬币 A ，若为正面则选硬币 B ，若为反面则选硬币 C ；然后掷 A 选出的硬币（ B 或 C ），若为正面则记作 1，若为反面则记作 0，试验结束。\n",
    "\n",
    "独立重复试验 $n$ 次（这里 $n=10$），观测结果如下：\n",
    "\n",
    "$$1,1,0,1,0,0,1,0,1,1$$\n",
    "\n",
    "假设*只能观测到掷硬币的结果*，*不能观测掷硬币的过程*，问如何估计三硬币正面出现的概率，即三硬币模型的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解** 三硬币模型可以写作：\n",
    "\n",
    "$$\n",
    "\\begin{aligned} P(y | \\theta) &=\\sum_{z} P(y, z | \\theta)=\\sum_{z} P(z | \\theta) P(y | z, \\theta) \\\\ &=\\pi p^{y}(1-p)^{1-y}+(1-\\pi) q^{y}(1-q)^{1-y} \\end{aligned}\n",
    "\\tag{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，随机变量 $y$ 是**观测变量**，表示一次试验观测的结果是 1 或 0；随机变量 $z$ 是**隐变量**，表示未观测到的掷硬币 A 的结果；$\\theta=(\\pi,p,q)$ 是模型参数。这一模型是以上数据的生成模型。注意，*随机变量 $y$ 的数据可以观测，随机变量 $z$ 的数据不可观测*。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将观测数据表示为 $Y=(Y_1,Y_2,...,Y_n)^T$ ，未观测数据表示为 $Z=(Z_1,Z_2,...,Z_n)^T$，则观测数据的似然函数为：\n",
    "\n",
    "$$P(Y|\\theta)=\\sum_Z P(Z|\\theta)P(Y|Z,\\theta) \\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即\n",
    "\n",
    "$$\n",
    "P(Y | \\theta)=\\prod_{j=1}^{n}\\left[\\pi p^{y_{i}}(1-p)^{1-y_{j}}+(1-\\pi) q^{y_{j}}(1-q)^{1-y_{j}}\\right] \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑求模型参数 $\\theta=(\\pi,p,q)$，即：\n",
    "\n",
    "$$\\hat{\\theta}=arg \\max \\limits_{\\theta} logP(Y|\\theta) \\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个问题没有解析解，只有通过迭代的方法求解。EM 算法就是可以用于求解这个问题的一种迭代算法。下面给出针对以上问题的 EM 算法，其推导过程省略。\n",
    "\n",
    "EM 算法首先选取参数的初值，记作 $\\theta^{(0)}=(\\pi^{(0)},p^{(0)},q^{(0)})$ ，然后通过下面的步骤迭代计算参数的估计值，直至收敛为止。第 $i$ 次迭代参数的估计值为 $\\theta^{(i)}=(\\pi^{(i)},p^{(i)},q^{(i)})$。EM 算法的第 $i+1$ 次迭代如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E 步**：计算在模型参数 $\\pi^{(i)},p^{(i)},q^{(i)}$ 下观测数据 $y_i$ 来自掷硬币 B 的概率：\n",
    "\n",
    "$$\n",
    "\\mu^{(i+1)}=\\frac{\\pi^{(i)}\\left(p^{(i)}\\right)^{y_{j}}\\left(1-p^{(i)}\\right)^{1-y_{j}}}{\\pi^{(i)}\\left(p^{(i)}\\right)^{y_{j}}\\left(1-p^{(i)}\\right)^{1-y_{j}}+\\left(1-\\pi^{(i)}\\right)\\left(q^{(i)}\\right)^{y_{j}}\\left(1-q^{(i)}\\right)^{1-y_{j}}}\n",
    "\\tag{5}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M 步**：计算模型参数的新估计值\n",
    "\n",
    "$$ \\pi^{(i+1)}=\\frac{1}{n} \\sum_{j=1}^{n} \\mu_{j}^{(i+1)} \\tag{6}$$\n",
    "\n",
    "$$ p^{(i+1)}=\\frac{\\sum_{j=1}^{n} \\mu_{j}^{(i+1)} y_{j}}{\\sum_{j=1}^{n} \\mu_{j}^{(i+1)}} \\tag{7}$$\n",
    "\n",
    "$$ q^{(i+1)}=\\frac{\\sum_{j=1}^{n}\\left(1-\\mu_{j}^{(i+1)}\\right) y_{j}}{\\sum_{j=1}^{n}} \\tag{8}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行数字计算。假设模型参数的初值取为：\n",
    "\n",
    "$$\\pi^{(0)}=0.5,\\quad p^{(0)}=0.5,\\quad q^{(0)}=0.5$$\n",
    "\n",
    "由式 (5)，对 $y_j=1$ 与 $y_j=0$ 均有 $\\mu_j^{(1)}=0.5$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用迭代公式 (6-8) ，得到：\n",
    "\n",
    "$$\\pi^{(1)}=0.5,\\quad p^{(1)}=0.6,\\quad q^{(1)}=0.6$$\n",
    "\n",
    "由式 (5)，\n",
    "\n",
    "$$\\mu_j^{(2)}=0.5, \\quad j=1,2,...,10$$\n",
    "\n",
    "继续迭代，得：\n",
    "\n",
    "$$\\pi^{(2)}=0.5,\\quad p^{(2)}=0.6,\\quad q^{(2)}=0.6$$\n",
    "\n",
    "可以看到参数以及收敛，于是得到模型参数 $\\theta$ 的极大似然估计：\n",
    "\n",
    "$$\\hat{\\pi}=0.5, \\quad \\hat{p}=0.6 \\quad \\hat{q}=0.6$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pi = 0.5$ 表示硬币 A 是均匀的，这一结果容易理解。\n",
    "\n",
    "如果取初值$\\pi^{(0)}=0.5,\\quad p^{(0)}=0.6,\\quad q^{(0)}=0.7$，那么得到的模型参数的极大似然估计是 $\\hat{\\pi}=0.4064, \\quad \\hat{p}=0.5368 \\quad \\hat{q}=0.6432$ 。这就是说，*EM 算法与初值的选择有关，选择不同的初值可能得到不同的参数估计值*。\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般地，用 $Y$ 表示**观测随机变量**的数据，$Z$ 表示**隐随机变量**的数据。$Y$ 和 $Z$ 连在一起称为`完全数据`（complete-data），观测数据 $Y$ 又称为`不完全数据`（incomplete-data）。假设给定观测数据 $Y$，其概率分布是 $P(Y|\\theta)$，其中 $\\theta$ 是需要估计的模型参数，那么不完全数据 $Y$ 的似然函数是 $P(Y|\\theta)$，对数似然函数为 $L(\\theta)=\\log P(Y|\\theta)$；假设 $Y$ 和 $Z$ 的联合概率分布是 $P(Y,Z|\\theta)$，那么完全数据的对数似然函数是 $\\log P(Y,Z|\\theta)$。\n",
    "\n",
    "EM 算法通过迭代求 $L(\\theta)=\\log P(Y|\\theta)$ 的极大似然估计。每次迭代包含两步：E 步，求期望；M 步，求极大化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EM 算法\n",
    "\n",
    "**输入**：观测变量数据 $Y$，隐变量数据 $Z$，联合分布 $P(Y,Z|\\theta)$，条件分布 $P(Z|Y.\\theta)$；\n",
    "\n",
    "**输出**；模型参数 $\\theta$。\n",
    "\n",
    "（1）选择参数的初值 $\\theta^{(0)}$，开始迭代；\n",
    "\n",
    "（2）**E 步**：记 $\\theta^{(i)}$ 为第 $i$ 次迭代参数 $\\theta$ 的估计值，在第 $i+1$ 次迭代的 E 步，计算：\n",
    "\n",
    "$$\n",
    "\\begin{aligned} Q\\left(\\theta, \\theta^{(i)}\\right) &=E_{Z}\\left[\\log P(Y, Z | \\theta) | Y, \\theta^{(i)}\\right] \\\\ &=\\sum_{Z} \\log P(Y, Z | \\theta) P\\left(Z | Y, \\theta^{(i)}\\right) \\end{aligned} \\tag{9}\n",
    "$$\n",
    "\n",
    "（3）**M 步**：求使 $Q(\\theta,\\theta^{(i)})$ 极大化的 $\\theta$，确定第 $i+1$ 次迭代的参数的估计值 $\\theta^{(i+1)}$：\n",
    "\n",
    "$$\\theta^{(i+1)}=arg \\max \\limits_{\\theta} Q(\\theta,\\theta^{(i)}) \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（4）重复第 (2-3) 步，直到收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "下面关于 EM 算法作几点说明：\n",
    "\n",
    "**步骤 (1)**  参数的初值可以任意选择，但需注意 **EM 算法对初值是敏感的**；\n",
    "\n",
    "**步骤 (2)**  E 步求 $Q(\\theta,\\theta^{(i)})$ 。Q 函数式中 Z 是未观测数据，Y 是观测数据。注意，$Q(\\theta,\\theta^{(i)})$ 的第 1 个表示要极大化的参数，第 2 个变元表示参数的当前估计值。每次迭代实际在求 Q 函数及其极大；\n",
    "\n",
    "**步骤 (3)** M 步求 $Q(\\theta,\\theta^{(i)})$ 的极大化，得到 $\\theta^{(i+1)}$，完成一次迭代 $\\theta^{(i)}\\to \\theta^{(i+1)}$，且每次迭代使似然函数增大或达到局部极值；\n",
    "\n",
    "**步骤 (4)** 给出停止迭代的条件，一般是对较小的正值 $\\varepsilon_1,\\varepsilon_2$，若满足：\n",
    "\n",
    "$$\n",
    "\\left\\|\\theta^{(i+1)}-\\theta^{(i)}\\right\\|<\\varepsilon_{1} \\quad or \\quad\\left\\|Q\\left(\\theta^{(i+1)}, \\theta^{(i)}\\right)-Q\\left(\\theta^{(i)}, \\theta^{(i)}\\right)\\right\\|<\\varepsilon_{2}\\tag{11}\n",
    "$$\n",
    "\n",
    "则停止迭代。\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式 (9) 的函数 $Q(\\theta,\\theta^{(i)})$ 是 EM 算法的核心，称为 Q 函数（Q function）。\n",
    "\n",
    "## Q 函数\n",
    "\n",
    "完全数据的对数似然函数 $\\log P(Y,Z|\\theta)$ 关于在给定观测数据 $Y$ 和当前参数 $\\theta^{(i)}$ 下对未观测数据 Z 的条件概率分布 $P(Z|Y,\\theta^{(i)})$ 的期望称为 `Q 函数`，即：\n",
    "\n",
    "$$\n",
    "Q\\left(\\theta, \\theta^{(i)}\\right)=E_{z}\\left[\\log P(Y, Z | \\theta) | Y, \\theta^{(i)}\\right]\n",
    "\\tag{11}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献\n",
    "\n",
    "[1] 周志华. 机器学习[M]. 北京: 清华大学出版社, 2016: 155-158."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
