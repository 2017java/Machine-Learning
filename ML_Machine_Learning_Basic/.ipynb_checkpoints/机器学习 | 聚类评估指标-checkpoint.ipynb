{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-聚类评估指标\" data-toc-modified-id=\"1.-聚类评估指标-1\">1. 聚类评估指标</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-外部评估指标\" data-toc-modified-id=\"1.1-外部评估指标-1.1\">1.1 外部评估指标</a></span><ul class=\"toc-item\"><li><span><a href=\"#RI-兰德指数\" data-toc-modified-id=\"RI-兰德指数-1.1.1\">RI 兰德指数</a></span><ul class=\"toc-item\"><li><span><a href=\"#ARI-调整兰德指数\" data-toc-modified-id=\"ARI-调整兰德指数-1.1.1.1\">ARI 调整兰德指数</a></span></li></ul></li><li><span><a href=\"#Jaccard-JC指数\" data-toc-modified-id=\"Jaccard-JC指数-1.1.2\">Jaccard JC指数</a></span></li><li><span><a href=\"#FMI-FMI指数\" data-toc-modified-id=\"FMI-FMI指数-1.1.3\">FMI FMI指数</a></span></li><li><span><a href=\"#MI-互信息\" data-toc-modified-id=\"MI-互信息-1.1.4\">MI 互信息</a></span><ul class=\"toc-item\"><li><span><a href=\"#NMI-归一化互信息\" data-toc-modified-id=\"NMI-归一化互信息-1.1.4.1\">NMI 归一化互信息</a></span></li><li><span><a href=\"#AMI-调整互信息\" data-toc-modified-id=\"AMI-调整互信息-1.1.4.2\">AMI 调整互信息</a></span></li></ul></li></ul></li><li><span><a href=\"#1.2-内部评估指标\" data-toc-modified-id=\"1.2-内部评估指标-1.2\">1.2 内部评估指标</a></span><ul class=\"toc-item\"><li><span><a href=\"#DBI-戴维森堡丁指数\" data-toc-modified-id=\"DBI-戴维森堡丁指数-1.2.1\">DBI 戴维森堡丁指数</a></span></li><li><span><a href=\"#DI-Dunn指数\" data-toc-modified-id=\"DI-Dunn指数-1.2.2\">DI Dunn指数</a></span></li><li><span><a href=\"#SC-轮廓系数\" data-toc-modified-id=\"SC-轮廓系数-1.2.3\">SC 轮廓系数</a></span></li></ul></li></ul></li><li><span><a href=\"#参考文献\" data-toc-modified-id=\"参考文献-2\">参考文献</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 聚类评估指标\n",
    "\n",
    "[Clustering performance evaluation](https://scikit-learn.org/dev/modules/clustering.html#clustering-evaluation)\n",
    "\n",
    "聚类性能度量亦称聚类“`有效性指标`”（validity index）。与监督学习中的[性能度量](https://blog.csdn.net/weixin_45488228/article/details/98896294)相似，对聚类结果，我们需通过某种性能度量来评估其好坏；另一方面，若明确了最终将要使用的性能度量，则可直接将其作为聚类过程的优化目标，从而更好地得到符合要求的聚类结果。\n",
    "\n",
    "聚类是将样本集D划分为若干互不相关的子集，即`样本簇`（类），而我们又希望聚类结果的“`簇内相似度`”（intra-cluster similarity）高且“`簇间相似度`”（intra-cluster similarity）低。\n",
    "\n",
    "聚类性能度量大致有两类，一类是将聚类结果与某个“`参考模型`”（reference model，样本含标签的）进行比较，称为“`外部指标`”（external index）;另一类是直接考察聚类结果而不利用任何参考模型，称为“`内部指标`”（internal index）。\n",
    "\n",
    "对数据集 $D=\\{x_1,x_2,\\cdots,x_n\\}$，假定通过聚类给出的 $k$ 个簇，划分为 $C=\\{C_1,C_2,\\cdots,C_k\\}$，参考模型给出的 $s$ 个簇划分为 $C^*=\\{C_1^*,C_2^*,\\cdots,C_s^*\\}$。相应地，令 $\\lambda$ 与 $\\lambda^*$ 分别表示 $C$ 与 $C^*$ 对应的簇标记向量。我们将样本两两配对考虑，定义：\n",
    "\n",
    "$$a=|SS|,\\quad SS=\\{(x_i,x_j)| \\lambda_i=\\lambda_j,\\lambda_i^*=\\lambda_j^*,i<j\\} \\tag{1}$$\n",
    "\n",
    "$$b=|SD|,\\quad SD=\\{(x_i,x_j)| \\lambda_i=\\lambda_j,\\lambda_i^*\\neq\\lambda_j^*,i<j\\} \\tag{2}$$\n",
    "\n",
    "$$c=|DS|,\\quad DS=\\{(x_i,x_j)| \\lambda_i\\neq\\lambda_j,\\lambda_i^*=\\lambda_j^*,i<j\\} \\tag{3}$$\n",
    "\n",
    "$$d=|DD|,\\quad DD=\\{(x_i,x_j)| \\lambda_i\\neq\\lambda_j,\\lambda_i^*\\neq\\lambda_j^*,i<j\\} \\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中集合 $SS$ 表示点 $i$ 和点 $j$ 在**聚类结果中处于同一个簇**，而**实际上这两个点也是处于同一个簇**的所有点的集合，相当于[混淆矩阵](https://blog.csdn.net/weixin_45488228/article/details/98896294)中的 TP；\n",
    "\n",
    "集合 $SD$ 表示点 $i$ 和点 $j$ 在**聚类结果中处于同一个簇**，而**实际上这两个点不处于同一个簇**的所有点的集合，相当于混淆矩阵中的 FP，...。\n",
    "\n",
    "由于每个样本对 $(x_i,x_j)(i<j)$ 仅能出现在一个集合中，因此有 $a+b+c+d=C_n^2=n(n-1)/2$ 成立。<sup>[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 外部评估指标\n",
    "\n",
    "基于公式（1-4）可以导出下面常用的聚类性能度量`外部指标`："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RI 兰德指数\n",
    "\n",
    "Rand Index 兰德指数\n",
    "\n",
    "$$RI=\\frac{(a+b)}{C_n^2}\\tag{}$$\n",
    "\n",
    "#### ARI 调整兰德指数\n",
    "\n",
    "Adjuested Rand Index 调整兰德指数\n",
    "\n",
    "[sklearn.metrics.adjusted_rand_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score)\n",
    "\n",
    "$$ARI=\\frac{RI-E(RI)}{max(RI)-E(RI)}\\tag{5}$$\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "- **Random (uniform) label assignments have a ARI score close to 0.0** for any value of n_clusters and n_samples (which is not the case for raw Rand index or the V-measure for instance).\n",
    "\n",
    "- **Bounded range [-1, 1]:** negative values are bad (independent labelings), similar clusterings have a positive ARI, 1.0 is the perfect match score.\n",
    "\n",
    "- **No assumption is made on the cluster structure:** can be used to compare clustering algorithms such as k-means which assumes isotropic blob shapes with results of spectral clustering algorithms which can find cluster with “folded” shapes.\n",
    "\n",
    "**Drawbacks**\n",
    "\n",
    "- Contrary to inertia, **ARI requires knowledge of the ground truth classes** while is almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting).\n",
    "\n",
    "However ARI can also be useful in a purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection (TODO).\n",
    "\n",
    "### Jaccard JC指数\n",
    "\n",
    "Jaccard Coefficient\n",
    "\n",
    "$$JC=\\frac{a}{a+b+c}\\tag{6}$$\n",
    "\n",
    "### FMI FMI指数\n",
    "\n",
    "Fowlkes and Mallows Index\n",
    "\n",
    "[sklearn.metrics.fowlkes_mallows_score](https://scikit-learn.org/dev/modules/clustering.html#fowlkes-mallows-scores)\n",
    "\n",
    "$$FMI=\\sqrt{\\frac{a}{a+b}\\cdot\\frac{a}{a+c}}\\tag{7}$$\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "- **Random (uniform) label assignments have a FMI score close to 0.0** for any value of n_clusters and n_samples (which is not the case for raw Mutual Information or the V-measure for instance).\n",
    "\n",
    "- **Upper-bounded at 1:** Values close to zero indicate two label assignments that are largely independent, while values close to one indicate significant agreement. Further, values of exactly 0 indicate purely independent label assignments and a FMI of exactly 1 indicates that the two label assignments are equal (with or without permutation).\n",
    "\n",
    "- **No assumption is made on the cluster structure:** can be used to compare clustering algorithms such as k-means which assumes isotropic blob shapes with results of spectral clustering algorithms which can find cluster with “folded” shapes.\n",
    "\n",
    "**Drawbacks**\n",
    "\n",
    "- Contrary to inertia, **FMI-based measures require the knowledge of the ground truth classes** while almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MI 互信息\n",
    "\n",
    "Mutual Information 互信息\n",
    "\n",
    "[sklearn.metrics.mutual_info_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html#sklearn.metrics.mutual_info_score)\n",
    "\n",
    "对数据集 $D=\\{x_1,x_2,\\cdots,x_n\\}$，假定通过聚类给出的 $k$ 个簇，划分为 $C=\\{C_1,C_2,\\cdots,C_k\\}$，参考模型给出的 $s$ 个簇划分为 $C^*=\\{C_1^*,C_2^*,\\cdots,C_s^*\\}$。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{MI(C,C^*)} &= {\\sum_{i=1}^k \\sum_{j=1}^s P(C_i,C_j^*)log \\frac{P(C_i\\cap C_j^*)}{P(C_i)P(C_j^*)}} \\\\\n",
    "&= {\\sum_{i=1}^k \\sum_{j=1}^s \\frac{|C_i\\cap C_j^*|}{n}log\\frac{n\\cdot|C_i\\cap C_j^*|}{|C_i||C_j^*|}} \\\\\n",
    "\\end{aligned}\\tag{8}\n",
    "$$\n",
    "\n",
    "$P(C_i),P(C_j^*),P(C_i\\cap C_j^*)$ 可以分别看作样本属于聚类簇 $C_i$ ，属于类 $C_j^*$ 以及同时属于两者的概率。\n",
    "\n",
    "定义熵 H：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(C)& =-\\sum_{i=1}^kP(C_i)log P(C_i)\\\\\n",
    "& = -\\sum_{i=1}^k \\frac{|C_i|}{n}log(\\frac{|C_i|}{n})\\\\\n",
    "\\end{aligned}\\tag{9}\n",
    "$$\n",
    "\n",
    "给定簇信息 $C^*$ 的前提条件下，类别信息 $C$ 的增加量，或者说其不确定度的减少量，直观的，可以写出如下形式：\n",
    "\n",
    "$$MI(C,C^*)=H(C)-H(C|C^*)\\tag{10}$$\n",
    "\n",
    "- 互信息的最小值为 0, 当类簇相对于类别只是随机的, 也就是说两者独立的情况下, $C$ 对于 $C^*$ 未带来任何有用的信息；\n",
    "\n",
    "- 如果得到的 $C$ 与 $C^*$ 关系越密切, 那么 $MI(C,C^*)$ 值越大. 如果 $C$ 完整重现了 $C^*$ , 此时互信息最大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 当 $k=n$ 时,即类簇数和样本个数相等,MI 也能达到最大值。所以 MI 也存在和纯度类似的问题,即它并不对簇数目较大的聚类结果进行惩罚,因此也不能在其他条件一样的情况下,对簇数目越小越好的这种期望进行形式化。\n",
    "\n",
    "#### NMI 归一化互信息\n",
    "\n",
    "Normalized Mutual Information 归一化互信息\n",
    "\n",
    "[sklearn.metrics.normalized_mutual_info_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html#sklearn.metrics.normalized_mutual_info_score)\n",
    "\n",
    "NMI 则可以解决上述问题,因为熵会随着簇的数目的增长而增大。当  $k=n$ 时, $H(C)$ 会达到其最大值 $log(n)$ , 此时就能保证 NMI 的值较低。之所以采用 $\\frac{1}{2} H(C)+H(C^*)$ 作为分母,是因为它是 $MI(C,C^*)$ 的紧上界, 因此可以保证 $NMI\\in[0,1]$ 。\n",
    "\n",
    "$$NMI(C,C^*)=\\frac{2\\times MI(C,C^*)}{H(C)+H(C^*)}\\tag{11}$$\n",
    "\n",
    "#### AMI 调整互信息\n",
    "\n",
    "Adjusted Mutual Information 调整互信息\n",
    "\n",
    "[sklearn.metrics.adjusted_mutual_info_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score)\n",
    "\n",
    "$$AMI(C,C^*)=\\frac{MI(C,C^*)-E(MI(C,C^*))}{avg(H(C),H(C^*))-E[MI(C,C^*)]}\\tag{12}$$\n",
    "\n",
    "令 $a_i=|C_i|,b_j=|C_J^*|$ ，则 $E[MI(C,C^*)]$ 为：\n",
    "\n",
    "$$E[MI(C,C^*)] = \\sum_{i=1}^{|C|}\\sum_{j=1}^{|C^*|} \\sum_{n_{ij}=(a_i+b_j-n)^+}^{min(a_i,b_j)} \\frac{n_{ij}}{n}log(\\frac{n\\cdot n_{ij}}{a_i b_j}) \\frac{a_i!b_j!(n-a_i)!(n=b_j)!}{n!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!(n-a_i-b_j+n_{ij})!} \\tag{13}$$\n",
    "\n",
    "当 log 取 2 为底时，单位为 bit，取 e 为底时单位为 nat。<sup>[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 内部评估指标\n",
    "\n",
    "考虑聚类结果的 $k$ 个簇划分 $C=\\{C_1,C_2,\\cdots,C_k\\}$，其中 $dist(\\cdot,\\cdot)$ 用于计算两个样本之间的距离（2. 距离计算），$\\mu$ 代表簇 $C$ 的中心点 $\\mu=\\frac{1}{k}\\sum_{1\\leq i\\leq k} x_i$。\n",
    "\n",
    "定义：\n",
    "\n",
    "簇 $C$ 内样本间的`平均距离` $avg(C)$：\n",
    "\n",
    "$$avg(C)=C_k^2\\sum_{1\\leq i\\leq j \\leq k} dist(x_i,x_j) \\tag{14}$$\n",
    "\n",
    "簇 $C$ 内样本间的`最远距离` $diam(C)$：\n",
    "\n",
    "$$diam(C)=max_{1\\leq i\\leq j \\leq k}dist(x_i,x_j) \\tag{15}$$\n",
    "\n",
    "簇 $C_i$ 与簇 $C_j$ `最近的样本间距离` $d_{min}(C_i,C_j)$：\n",
    "\n",
    "$$d_{min}(C_i,C_j)=min_{x_i \\in C_i,x_j \\in C_j} dist(x_i,x_j) \\tag{16}$$\n",
    "\n",
    "簇 $C_i$ 与簇 $C_j$ `中心点间的距离` $d_{cen}(C_i,C_j)$：\n",
    "\n",
    "$$d_{cen}(C_i,C_j)=dist(x_i,x_j) \\tag{17}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBI 戴维森堡丁指数\n",
    "\n",
    "Davies-Bouldin Index 戴维森堡丁指数，越小越好。\n",
    "\n",
    "[sklearn.metrics.davies_bouldin_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score)\n",
    "\n",
    "$$DBI=\\frac{1}{k}\\sum_{i=1}^k \\max \\limits_{j\\neq i}(\\frac{avg(C_i)+avg(C_j)}{d_{cen}(\\mu_i,\\mu_j)}) \\tag{18}\n",
    "$$\n",
    "\n",
    "### DI Dunn指数\n",
    "\n",
    "Dunn Index，越大越好。\n",
    "\n",
    "$$DI= \\min \\limits_{1\\leq i\\leq k} \\bigg\\{ \\min \\limits_{j \\neq i}\\bigg(\\frac{d_{min}(C_i,C_j)}{\\max \\limits_{1 \\leq l \\leq k} diam(C_l)}\\bigg) \\bigg\\} \\tag{19}\n",
    "$$\n",
    "\n",
    "### SC 轮廓系数\n",
    "\n",
    "Silhouette Coefficient 轮廓系数\n",
    "\n",
    "[sklearn.metrics.silhouette_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score)\n",
    "\n",
    "$$S=\\frac{b-a}{max(a,b)} \\tag{20}$$\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "- The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.\n",
    "\n",
    "- The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.\n",
    "\n",
    "**Drawbacks**\n",
    "\n",
    "- The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.【不适用于紧凑、密集的数据，DBSCAN的带噪声的数据】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献\n",
    "\n",
    "[1] 周志华. 机器学习[M]. 北京: 清华大学出版社, 2016: 197-199.\n",
    "\n",
    "[2] Gan Pan.[ML] 聚类评价指标[EB/OL].https://zhuanlan.zhihu.com/p/53840697, 2019-06-28."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
