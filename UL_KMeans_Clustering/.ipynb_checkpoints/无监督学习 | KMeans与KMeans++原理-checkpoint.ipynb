{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-原型聚类\" data-toc-modified-id=\"1.-原型聚类-1\">1. 原型聚类</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-KMeans\" data-toc-modified-id=\"1.1-KMeans-1.1\">1.1 KMeans</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1.1-最小化成本函数\" data-toc-modified-id=\"1.1.1-最小化成本函数-1.1.1\">1.1.1 最小化成本函数</a></span></li><li><span><a href=\"#1.1.2-实例\" data-toc-modified-id=\"1.1.2-实例-1.1.2\">1.1.2 实例</a></span></li></ul></li><li><span><a href=\"#1.2-KMeans++\" data-toc-modified-id=\"1.2-KMeans++-1.2\">1.2 KMeans++</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.2.1-KMeans++-初始化实例\" data-toc-modified-id=\"1.2.1-KMeans++-初始化实例-1.2.1\">1.2.1 KMeans++ 初始化实例</a></span></li></ul></li></ul></li><li><span><a href=\"#参考资料\" data-toc-modified-id=\"参考资料-2\">参考资料</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相关文章：\n",
    "\n",
    "[机器学习 | 目录](https://blog.csdn.net/weixin_45488228/article/details/99691709)\n",
    "\n",
    "[机器学习 | 聚类评估指标](https://blog.csdn.net/weixin_45488228/article/details/100549820)\n",
    "\n",
    "[机器学习 | 距离计算](https://blog.csdn.net/weixin_45488228/article/details/100593643)\n",
    "\n",
    "无监督学习 | KMeans之Skleaen实现：电影评分聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 原型聚类\n",
    "\n",
    "`原型聚类`亦称“基于原型的聚类”（prototypr-based clustering）。此类算法假设聚类结构能通过一组原型刻画，在现实聚类任务重及其常用。通常情形下，**算法先对原型进行初始化，然后对原型进行迭代更新求解**。采用不同的原型表示、不同的求解方式，将产生不同的算法，如 KMeans、LVQ、高斯混合。下面介绍 KMeans 算法，我们将在下一篇文章中介绍高斯混合算法。\n",
    "\n",
    "> “原型”是指样本空间具有代表性的点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 KMeans\n",
    "\n",
    "给定样本集 $D={x_1,x_2,\\cdots,x_m}$，“`$k$ 均值`”（k-means）算法针对聚类所得簇划分 $C={C_1,C_2,\\cdots,C_k}$ 最小化平方误差（残差平方和 $S_E$）：\n",
    "\n",
    "$$E=\\sum_{i=1}^k \\sum_{x\\in C_i}\\|x-\\mu_i\\|_2^2 \\tag{1}$$\n",
    "\n",
    "其中 $\\mu_i=\\frac{1}{|C_i|}\\sum_{x\\in C_i}x$ 是簇 $C_i$ 的均值向量。直观来看，式 (1) 在一定程度上刻画了簇内`样本围绕簇均值向量的紧密程度`，$E$ 值越小则簇内样本相似度越高。\n",
    "\n",
    "### 1.1.1 最小化成本函数\n",
    "\n",
    "最小化式 (1) 并不容易，找到它的最优解需考察样本集 $D$ 所有可能的簇划分，这是一个 NP 难问题。因此，k均值算法采用了`贪心策略`，通过迭代优化来近似求解式 (2) 。算法流程如下图所示：\n",
    "\n",
    "<img style=\"float:center\" src=\"https://x1a-alioss.oss-cn-shenzhen.aliyuncs.com/Screen Shot 2019-09-07 at 22.14.47.png\" width=\"520\" >\n",
    "\n",
    "<center>图1 k 均值算法</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中第 1 行对**均值向量进行初始化**，在第 4-8 行对**当前簇划分迭代更新**，第 9-16 行对**均值向量迭代更新**，若迭代更新后均值结果保持不变，则在第 18 行对当前簇划分结果返回。\n",
    "\n",
    "> 为避免运行时间过长，通常设置一个`最大运行轮数`（max_iter）或`最小调整幅度阈值`，若达到最大轮数或调整幅度小于阈值，则停止运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 实例\n",
    "\n",
    "下面对西瓜数据集为例演示 k 均值算法的学习过程。为方便叙述，我们将变好为 $i$ 的样本称为 $x_i$，这是一个包含“密度”与“含糖率”两个属性值的二维向量。\n",
    "\n",
    "<center>表1 西瓜数据集</center>\n",
    "\n",
    "<img style=\"float:center\" src=\"https://x1a-alioss.oss-cn-shenzhen.aliyuncs.com/Screen Shot 2019-09-07 at 22.37.51.png\" width=\"520\" >\n",
    "\n",
    "假定`聚类簇数`（n_clusters）$k=3$ ，算法开始时随机选取三个样本 $x_6,x_{12},x_{27}$ 作为`初始均值向量`，即：\n",
    "\n",
    "$$\\mu_1=(0.403;0.237),\\mu_2=(0.343;0.0999),\\mu_3=(0.532;0.472)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考察样本 $x_1=(0.697;0.460)$ ，它与当前均值向量 $\\mu_1,\\mu_2,\\mu_3$ 的距离分别为 0.369，0.506，0.166，因此 $x_1$ 将划入簇 $C_3$ 中。类似的，对数据集中的所有样本考虑一遍后，可得当前簇划分为：\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "C_1& =\\{x_5,x_6,x_7,x_8,x_9,x_{10},x_{13},x_{14},x_{15},x_{17},x_{18},x_{19},x_{20},x_{23}\\}; \\\\\n",
    "C_2& = \\{x_{11},x_{12},x_{16}\\};\\\\\n",
    "C_3& = \\{x_{1},x_{2},x_{3},x_{4},x_{21},x_{22},x_{24},x_{25},x_{26},x_{27},x_{28},x_{29},x_{30};\\} \\\\\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是，可从 $C_1、C_2、C_3$ 分别求出新的均值向量：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_{1}^{\\prime}=(0.473 ; 0.214), \\boldsymbol{\\mu}_{2}^{\\prime}=(0.394 ; 0.066), \\boldsymbol{\\mu}_{3}^{\\prime}=(0.623 ; 0.388)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新当前均值向量后，不断重复上述过程，如下图所示，第五轮迭代产生的结果与第四轮迭代相同，于是算法停止，得到最终的簇划分：<sup>[1]</sup>\n",
    "\n",
    "<img style=\"float:center\" src=\"https://x1a-alioss.oss-cn-shenzhen.aliyuncs.com/Screen Shot 2019-09-07 at 23.06.19.png\" width=\"520\" >\n",
    "\n",
    "<center>图2 k 均值算法 4 轮迭代后的簇划分</center>\n",
    "\n",
    "标准 KMeans 的聚类结果受初始均值向量的影响，初始点不同，则聚类结果就有可能不同，因此可以通过`多次随机初始化`（n_init）聚类中心最终选取最优结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 KMeans++\n",
    "\n",
    "由于 KMeans 算法的分类结果会收到初始点的选取而有所区别，因此提出了标准 KMeans 的改进 KMeans++。\n",
    "\n",
    "其改进在于对初始均值向量的选择，其他步骤同标准 KMeans 相同。初始均值向量选取的基本思路是：初始的聚类中心之间的相互距离要尽量远。\n",
    "\n",
    "**初始均值向量选取如下：**\n",
    "\n",
    "步骤一：随机选取一个样本作为第一个聚类中心 c1；\n",
    "\n",
    "步骤二：\n",
    "\n",
    "- 计算每个样本与当前已有类聚中心最短距离（即与最近一个聚类中心的距离），用 D(x)表示；\n",
    "\n",
    "- 接着计算每个样本被选为下一个聚类中心的概率:\n",
    "\n",
    "$$\\frac{D(x)^2}{\\sum_{x\\in X}D(x)^2} \\tag{2}$$\n",
    "\n",
    "这个值越大，表示被选取作为聚类中心的概率较大；\n",
    "\n",
    "- 最后，用轮盘法选出下一个聚类中心；\n",
    "\n",
    "步骤三：重复步骤二，直到选出 k 个聚类中心。<sup>[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 KMeans++ 初始化实例\n",
    "\n",
    "假设经过步骤一后 6 号点被选择为第一个初始聚类中心，那在进行步骤二时每个样本的 $D(x)$ 和被选择为第二个聚类中心的概率 $P(x)$ 如下表所示：\n",
    "\n",
    "<center>表2 第二个聚类中心的选择</center>\n",
    "\n",
    "<img style=\"float:center\" src=\"https://x1a-alioss.oss-cn-shenzhen.aliyuncs.com/8166116-64b4ce3467bede92.png\" width=\"520\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中的 $P(x)$ 就是每个样本被选择为下一个聚类中心的概率。因此 $P(x)$ 可以看作 PDF，Sum 可以看作 CDF，是 $P(x)$ 的累加和，用于轮盘法选择出第二个聚类中心。\n",
    "\n",
    "`轮盘法`方法就是随机产生一个 0～1 之间的随机数，判断随机数属于 Sum 行的哪个区间，那么该区间对应的序号就是被选择出来的第二个聚类中心了。例如 1 号点的区间为 $[0,0.2)$ ，2 号点的区间为 $[0.2,0.525)$。\n",
    "\n",
    "从表中可以直观的看到第二个初始聚类中心有 90% 的概率落在 1～4 号点。而这 4 个点正好是离第一个聚类中心 6 号点较远的四个点。这也验证了 KMeans++ 的思想：即离当前已有聚类中心较远的点有更大的概率被选为下一个聚类中心。\n",
    "\n",
    "当 k 值大于 2 时，每个样本会有多个距离，需要取最小的那个距离作为 $D(x)$。\n",
    "\n",
    "重复步骤 2 直到选出 k 个聚类中心，并利用这 k 个初始聚类中心来运行标准 KMeans 算法。<sup>[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考资料\n",
    "\n",
    "[1] 周志华. 机器学习[M]. 北京: 清华大学出版社, 2016: 202-205.\n",
    "\n",
    "[2] 寒杰士.[ML] K-means与K-means++[EB/OL].https://www.cnblogs.com/wang2825/articles/8696830.html, 2018-04-02.\n",
    "\n",
    "[3] 0过把火0.[ML] K-means++[EB/OL].https://www.jianshu.com/p/680dbffad345, 2018-10-19."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
