# Taxi-v2

当你运行 main.py 时，你在 Agent.py 中指定的智能体会与环境互动 20,000 个阶段。互动详情在 Monitor.py 中指定，它会返回两个变量：avg_rewards 和 best_avg_reward。

avg_rewards 是一个双队列，其中 avg_rewards[i] 是智能体从阶段 i+1 到阶段 i+100（含）收集的平均（未折扣）回报。例如，avg_rewards[0] 是智能体在前 100 个阶段中收集的平均回报。

best_avg_reward 是 avg_rewards 中的最大项。这是你在确定智能体在该任务中的效果时应该使用的最终得分。

**你的任务是修改 Agents.py 文件以改进智能体的性能。**

使用 __init__() 方法定义任何所需的实例变量。目前，我们定义了智能体可以采取的动作数量 (nA) ，并将动作值 (Q) 初始化为空的数组字典。你可以随意添加更多实例变量；例如，如果智能体使用 Epsilon 贪婪策略选择动作，你可能会发现有必要定义 ε 的值。

**select_action()** 方法将环境状态作为输入，并返回智能体选择的动作。我们提供的默认代码会随机地选择动作。

**step()** 方法将（state、action、reward、next_state）元组以及 done 变量作为输入，如果 ε 已结束，该变量将为 True。默认代码（你肯定需要更改！）会使上个状态动作对的值加一。你应该更改此方法，以使用抽取的经验元组更新智能体对问题的了解。
修改该函数后，你只需运行 python main.py 以测试新智能体。

虽然你可以实现你所选的任何算法，但是注意，你可以通过使用我们在课程中介绍的一些方法达到满意的性能。
评估性能

OpenAI Gym 将“解决”该任务定义为在连续尝试 100 次以上之后平均回报为 9.7。

虽然我们不会给此迷你项目打分，但是，建议你在连续尝试 100 次以上之后至少达到 9.1（best_avg_reward > 9.1）。