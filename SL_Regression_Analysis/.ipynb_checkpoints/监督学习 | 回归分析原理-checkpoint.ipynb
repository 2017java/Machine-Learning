{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#回归分析\" data-toc-modified-id=\"回归分析-1\">回归分析</a></span></li><li><span><a href=\"#1.-线性回归\" data-toc-modified-id=\"1.-线性回归-2\">1. 线性回归</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-基本形式\" data-toc-modified-id=\"1.1-基本形式-2.1\">1.1 基本形式</a></span></li><li><span><a href=\"#1.2-最小二乘法推导[2]\" data-toc-modified-id=\"1.2-最小二乘法推导[2]-2.2\">1.2 最小二乘法推导<sup>[2]</sup></a></span></li><li><span><a href=\"#1.3-计算复杂度\" data-toc-modified-id=\"1.3-计算复杂度-2.3\">1.3 计算复杂度</a></span></li></ul></li><li><span><a href=\"#2.-梯度下降\" data-toc-modified-id=\"2.-梯度下降-3\">2. 梯度下降</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1--误差函数\" data-toc-modified-id=\"2.1--误差函数-3.1\">2.1  误差函数</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1.1--平均绝对误差-MAE\" data-toc-modified-id=\"2.1.1--平均绝对误差-MAE-3.1.1\">2.1.1  平均绝对误差 MAE</a></span></li><li><span><a href=\"#2.1.2--均方误差-MSE\" data-toc-modified-id=\"2.1.2--均方误差-MSE-3.1.2\">2.1.2  均方误差 MSE</a></span></li></ul></li></ul></li><li><span><a href=\"#3.-多项式回归\" data-toc-modified-id=\"3.-多项式回归-4\">3. 多项式回归</a></span></li><li><span><a href=\"#4.-正则线性模型\" data-toc-modified-id=\"4.-正则线性模型-5\">4. 正则线性模型</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1-套索回归（L1正则化）\" data-toc-modified-id=\"4.1-套索回归（L1正则化）-5.1\">4.1 套索回归（L1正则化）</a></span></li><li><span><a href=\"#4.2-岭回归（L2正则化）\" data-toc-modified-id=\"4.2-岭回归（L2正则化）-5.2\">4.2 岭回归（L2正则化）</a></span></li><li><span><a href=\"#4.3-弹性网络回归（L1&amp;L2）\" data-toc-modified-id=\"4.3-弹性网络回归（L1&amp;L2）-5.3\">4.3 弹性网络回归（L1&amp;L2）</a></span></li></ul></li><li><span><a href=\"#5.--神经网络回归\" data-toc-modified-id=\"5.--神经网络回归-6\">5.  神经网络回归</a></span></li><li><span><a href=\"#6.-Sklearn-实现\" data-toc-modified-id=\"6.-Sklearn-实现-7\">6. Sklearn 实现</a></span></li><li><span><a href=\"#参考资料\" data-toc-modified-id=\"参考资料-8\">参考资料</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相关文章：[机器学习 | 回归评估指标](https://blog.csdn.net/weixin_45488228/article/details/98897061)\n",
    "\n",
    "[五种回归方法的比较](https://www.cnblogs.com/jin-liang/p/9551759.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回归分析\n",
    "\n",
    "在统计学中，回归分析（regression analysis）指的是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。回归分析按照涉及的变量的多少，分为一元回归和多元回归分析；按照因变量的多少，可分为简单回归分析和多重回归分析；按照自变量和因变量之间的关系类型，可分为线性回归分析和非线性回归分析。\n",
    "\n",
    "常见的方法有：\n",
    "\n",
    "1. Linear Regression线性回归\n",
    "\n",
    "2. Logistic Regression逻辑回归\n",
    "\n",
    "3. Polynomial Regression多项式回归\n",
    "\n",
    "4. Stepwise Regression逐步回归\n",
    "\n",
    "5. Ridge Regression岭回归\n",
    "\n",
    "6. Lasso Regression套索回归\n",
    "\n",
    "7. ElasticNet 弹性网络回归\n",
    "\n",
    "我们将对以上部分回归方法的原理进行介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 线性回归\n",
    "\n",
    "`线性回归`，又称**普通最小二乘法**（Ordinary Least Squares, OLS），是回归问题最简单也最经典的线性方法。线性回归需按照参数 w 和 b，使得对训练集的预测值与真实的回归目标值 y 之间的**均方误差**（MSE）最小。\n",
    "\n",
    "均方误差（[Mean Squared Error](https://blog.csdn.net/weixin_45488228/article/details/98897061)）是预测值与真实值之差的平方和除以样本数。\n",
    "\n",
    "线性回归没有参数，这是一个有点，但也因此无法控制模型的复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 基本形式 \n",
    "\n",
    "**线性回归预测模型：**\n",
    "\n",
    "$$f(x)=w_1 x_1 + w_2 x_2 + \\cdot \\cdot \\cdot + w_n x_n + b \\tag{1}$$\n",
    "\n",
    "- $f(x)$ 是预测值\n",
    "\n",
    "- $n$ 是特征的数量\n",
    "\n",
    "- $x_i$ 是第 $i$ 个特征值\n",
    "\n",
    "- 偏置项 $b$ 以及特征权重 $w_1, w_2,\\cdot \\cdot \\cdot ,w_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这可以用更为简介的向量化表示。\n",
    "\n",
    "**线性回归预测模型（向量化）：**\n",
    "\n",
    "$$\n",
    " \\begin{align*}  \n",
    "   f(x) &= w^T \\cdot x + b \\tag{2} \\\\  \n",
    "  &= \\hat{w}^T \\cdot x \\tag{3}\\\\   \n",
    " \\end{align*}\n",
    "$$\n",
    "\n",
    "-  $w=(w_1;w_2;...;w_n)$\n",
    "\n",
    "- $w$ 和 $b$ 学习得到，模型就得以确定\n",
    "\n",
    "- $\\hat{w}=(w;b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用`最小二乘法`最小化成本函数 ，可以得出 $\\hat{w}$ 的计算方程（标准方程）：<sup>[1]\n",
    "\n",
    "$$\\hat{w}^* = (X^T \\cdot X)^{(-1)} \\cdot X^T \\cdot y \\tag{4}$$\n",
    "\n",
    "- $\\hat{w}^*$ 是使成本函数 MSE 最小化的 $\\hat{w}$ 值\n",
    "\n",
    "- $y$ 是包含 $y^{(1)}$ 到 $y^{(m)}$ 的目标值向量\n",
    "\n",
    "则最终学得的`多元线性回归模型`为：\n",
    "\n",
    "$$f(\\hat{x}_i)=\\hat{x}_i(X^TX)^{-1}X^Ty \\tag{5}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 最小二乘法推导<sup>[2]</sup>\n",
    "\n",
    "线性回归试图学得：\n",
    "\n",
    "$$f(x_i)=wx_i+b, 使得 f(x_i)\\simeq y_i \\tag{6}$$\n",
    "\n",
    "\n",
    "如何确定 $w$ 和 $b$ 呢？关键在于如何衡量 $f(x)$ 与 $y$ 之间的差别。\n",
    "\n",
    "回想一下，训练模型就是设置模型参数知道模型最适应训练集的过程。要达到这个目的，我们首先需要知道怎么衡量模型对训练数据的拟合程度是好还是差，在 [机器学习 | 回归评估指标](https://blog.csdn.net/weixin_45488228/article/details/98897061) 里，我们了解到回归模型最常见的性能指标有均方误差（MSE）。因此以 MSE 为线性回归模型的成本函数，在训练线性回归模型时，我们需要找到最小化 MSE 的 $w$ 值 $w^*$，即：\n",
    "\n",
    "$$\n",
    " \\begin{align*}  \n",
    "   (w^*,b^*) &= \\mathop{\\arg\\min}\\limits_{(w,b)}\\sum_{i=1}^m(f(x_i)-y_i)^2 \\\\  \n",
    "  &= \\mathop{\\arg\\min}\\limits_{(w,b)}\\sum_{i=1}^m(y_i-wx_i-b)^2  \\\\    \n",
    " \\end{align*} \\tag{7}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "均方误差有非常好的几何意义，它对应了常用的欧几里得距离（或简称欧氏距离，Euclidean distance），基于均方误差最小化来进行模型求解的方法称为`“最小二乘法”`（least square method），在线性回归中，最小二乘法就是试图找出一条直线，使得所有样本到线上的欧氏距离之和最小。\n",
    "\n",
    "求解 $w$ 和 $b$ 使得 $E_{(w,b)}=\\sum_{i=1}^m(y_i-wx_i-b)^2$ 最小化的过程，称为`线性回归模型的最小二乘“参数估计”`（parameter estimation），我们可以将$E_{(w,b)}$ 分别对 $w$ 和 $b$ 求偏导，得到：\n",
    "\n",
    "$$\n",
    " \\begin{align*}  \n",
    "   \\frac{\\partial E_{(w,b)}}{\\partial w} &= 2\\bigg(w\\sum_{i=1}^m x_i^2 - \\sum_{i=1}^m(y_i-b)x_i \\bigg)  \\tag{8} \\\\  \n",
    "  \\frac{\\partial E_{(w,b)}}{\\partial b} &= 2\\bigg(mb - \\sum_{i=1}^m(y_i-wx_i)\\bigg) \\tag{9}\\\\   \n",
    " \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后令公式 (6)、(7) 为零可以得到 $w$ 和 $b$ 最优解的闭式（closed-form）解：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "w & = \\frac{\\sum_{i=1}^my_i(x_i-\\bar{x})}{\\sum_{i=1}^mx_i^2 - \\frac{1}{m}\\big(\\sum_{i=1}^m x^i\\big)^2} \\tag{10} \\\\\n",
    "b & = \\frac{1}{m}\\sum_{i=1}^m(y_i-wx_i) \\tag{11} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "更一般的情形是对如有 $n$ 个属性的数据集 $D$ ，这时我们试图学得：\n",
    "\n",
    "$$f(x_i)=w^Tx_i+b, 使得 f(x_i)\\simeq y_i \\tag{12}$$\n",
    "\n",
    "这称为`多元线性回归`（multivariate linear regression）.\n",
    "\n",
    "类似的，可利用最小二乘法对 $w$ 和 $b$ 进行估计。为便于讨论，我们吧 $w$ 和 $b$ 转换为向量形式 $\\hat{w} = (w;b)$，相应的，吧数据集 $D$ 表示为一个 $m \\times (d+1)$ 大小的矩阵 $X$，其中每行对应与一个示例，该行前 $d$ 哥元素对应与示例的 $d$ 个属性值，最后一个元素恒为 1，即：\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\left( \\begin{array}{cc} \n",
    "x_{11} & x_{12} & \\cdots& x_{1n} & 1\\\\\n",
    "x_{21} & x_{22} & \\cdots\\ & x_{2n} & 1\\\\\n",
    "\\vdots & \\vdots & \\ddots\\ & \\vdots & \\vdots\\\\\n",
    "x_{m1} & x_{m2} & \\cdots& x_{mn} & 1\\\\\n",
    "\\end{array} \\right)\n",
    "=\n",
    "\\left( \\begin{array}{cc} \n",
    "x_{1n}^T & 1\\\\\n",
    "x_{2n}^T & 1\\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "x_{m}^T & 1\\\\\n",
    "\\end{array} \\right)\\tag{13}\n",
    "$$\n",
    "\n",
    "再把标记也写成向量形式 $y=(y_1;y_2;\\cdots;y_m)$，类似于公式 (7) ，有：\n",
    "\n",
    "$$w^* = \\mathop{\\arg\\min}\\limits_{\\hat{w}}(y-X\\hat{w})^T(y-X\\hat{w})\\tag{14}$$\n",
    "\n",
    "令 $E_{\\hat{w}}=(y-X\\hat{w})^T(y-X\\hat{w})$ ，对 $\\hat{w}$ 求导得到：\n",
    "\n",
    "$$\\frac{dE_{\\hat{w}}}{d\\hat{w}}=2X^T(X\\hat{w}-y)\\tag{15}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令上式为零可得 $\\hat{w}$ 的最优解的闭式接，但由于涉及矩阵逆的计算，比单变量情形要复杂一些，下面我们做一个简单的讨论。\n",
    "\n",
    "当 $X^TX$ 为满秩矩阵（full-rank matrix）或正定矩阵（positive definite matrix）时，令公式 (15) 为零可得`标准方程`：\n",
    "\n",
    "$$\\hat{w}^*=(X^TX)^{-1}X^Ty \\tag{16}$$\n",
    "\n",
    "其中 $(X^TX)^{-1}$ 是矩阵 $(X^TX)$ 的逆矩阵，令 $\\hat{x}_i=(x_i,1)$ ，则最终学得的`多元线性回归模型`为：\n",
    "\n",
    "$$f(\\hat{x}_i)=\\hat{x}_i(X^TX)^{-1}X^Ty \\tag{17}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 计算复杂度\n",
    "\n",
    "标准方程需对矩阵 $(X^TX)$ 求逆，这是一个 $n \\times n$的矩阵（ $n$ 是特征数量）。对这种矩阵求逆计算复杂度通常为 $O(n^{2.4})$ 到 $O(n^{3})$ 之间（取决于计算实现）。换句话说，如果将特征数量翻倍，那么计算时间将乘以大约 $2^{2.4}=5.3$ 倍到 $2^{3}=8$ 倍之间。<sup>[3]\n",
    "    \n",
    "> 特征数量较大时（例如 100 000）时，标准方程的计算将极其缓慢\n",
    "\n",
    "好的一面是，相对于训练集中的实例数量 $O(m)$ 来说，方程式线性的，所以能够有效的处理大量的训练集，只要内存足够。\n",
    "\n",
    "同样，线性回归模型一经训练（不论是标准方程还是其他算法），预测就非常快速，因为计算复杂度相对于想要预测的实例数量和特征数量来说，都是线性的。换句话说，对两倍的实例（或者是两倍的特征数）进行预测，大概需要两倍的时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 梯度下降\n",
    "\n",
    "`梯度下降`（Grandient Descent）：梯度下降是一种非常通用的优化算法，能够为大范围的问题找到最优解。梯度下降的中心思想就是迭代地调整参数从而使成本函数最小化。\n",
    "\n",
    "具体来说，首先使用一个随机的 $\\hat{w}$ 值（随机初始化），然后逐步改进，每次踏出一步，每一步都尝试降低一点成本函数（如 MSE），知道算法*收敛*出一个最小值。\n",
    "\n",
    "\n",
    "\n",
    "* 思想：对于一组数据，随机设置一条线，并向两个相反的方向移动，最后取误差变小的那个方向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1  误差函数\n",
    "\n",
    "梯度下降误差函数 Error Functions\n",
    "* $w_i \\rightarrow w_i-\\frac{\\partial}{\\partial w_i}Error$\n",
    "![3ed5765b4546834d46573b5168d1d116](线性回归 - Linear Regression.resources/C8938CE2-2EC3-428A-BD3C-9AD9047D5FA7.png)\n",
    "\n",
    "* * *\n",
    "\n",
    "### 2.1.1  平均绝对误差 MAE\n",
    "\n",
    "Mean Absolute Error\n",
    "\n",
    "* $Error = \\frac{1}{m}  \\sum|y-\\widehat{y}|$\n",
    "![fb548bb87e12796a4807d0061a984e08](线性回归 - Linear Regression.resources/82B1A561-8D41-4F9C-8677-98BCB89276C7.png)\n",
    "![3e9ffb2c781a180ae237a230a469a840](线性回归 - Linear Regression.resources/2B061F61-3EFA-4FAB-BABA-446A4F1647F4.png)\n",
    "\n",
    "### 2.1.2  均方误差 MSE\n",
    "\n",
    "Mean Squared Error\n",
    "\n",
    "* $Error = \\frac{1}{2m}  \\sum(y-\\widehat{y})^2$\n",
    "![6f492af4ae0e27ed217d35028b3f780e](线性回归 - Linear Regression.resources/Screen Shot 2019-07-19 at 16.19.23.png)\n",
    "![f67da5185b7dcb5fca03423bf3db3975](线性回归 - Linear Regression.resources/D9AB53B0-2373-49AD-8829-04A11EE61831.png)\n",
    "\n",
    "* * *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 多项式回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 正则线性模型\n",
    "**Lamda Parameter**:\n",
    "* Small: Choose Complex Model\n",
    "* Large: Choose Simpler Moder\n",
    "![d971145b418eacaf61ea42b5f9705a3f](线性回归 - Linear Regression.resources/062F8E59-828C-4E6D-8C76-5D1E52A938B5.png)\n",
    "![c1e465e11ce4f6f36c35e316073934ce](线性回归 - Linear Regression.resources/E9B8DABF-A90B-40AF-A1A4-093B24A05292.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 套索回归（L1正则化）\n",
    "* 适用于稀疏数据（如1000列数据中只有10列相关）\n",
    "![e11404ca400bd5380f190f4643819893](线性回归 - Linear Regression.resources/740850D9-B056-449D-AB6D-0150FAE76561.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 岭回归（L2正则化）\n",
    "* 适用于非稀疏数据\n",
    "![201db7d35ce6ac39c6d62ed465bd3ba8](线性回归 - Linear Regression.resources/17C773A0-5F68-4E37-8A56-52D57B88926A.png)\n",
    "\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 弹性网络回归（L1&L2）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.  神经网络回归\n",
    "\n",
    "Regression Neural Network\n",
    "\n",
    "* 为了使输出值为任意数（而不是0或1），需要删除最后一个激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sklearn 实现\n",
    "\n",
    "我们将使用线性回归根据体质指数 (BMI) 预测预期寿命。\n",
    "\n",
    "对于线性模型，我们将使用 sklearn.linear_model.LogisticRegression 类（[Sklearn 官方文档](https://scikit-learn.org/dev/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)）。\n",
    "\n",
    "多元线性回归："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[60.31564716]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Add import statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assign the dataframe to this variable.\n",
    "# TODO: Load the data\n",
    "bmi_life_data = pd.read_csv(\"data/bmi_and_life_expectancy.csv\")\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "#TODO: Fit the model and Assign it to bmi_life_model\n",
    "bmi_life_model = LinearRegression()\n",
    "bmi_life_model.fit(bmi_life_data[['BMI']], bmi_life_data[['Life expectancy']])\n",
    "\n",
    "# Make a prediction using the model\n",
    "# TODO: Predict life expectancy for a BMI value of 21.07931\n",
    "laos_life_exp = bmi_life_model.predict(np.array(21.07931).reshape(-1,1))\n",
    "laos_life_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load the data from the boston house-prices dataset \n",
    "boston_data = load_boston()\n",
    "x = boston_data['data']\n",
    "y = boston_data['target']\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "# TODO: Fit the model and assign it to the model variable\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "# Make a prediction using the model\n",
    "sample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n",
    "                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n",
    "                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]\n",
    "# TODO: Predict housing price for the sample_house\n",
    "prediction = model.predict(sample_house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考资料\n",
    "\n",
    "[1] 周志华. 机器学习[M]. 北京: 清华大学出版社, 2016: 53-56\n",
    "\n",
    "[2] Aurelien Geron, 王静源, 贾玮, 边蕤, 邱俊涛. 机器学习实战：基于 Scikit-Learn 和 TensorFlow[M]. 北京: 机械工业出版社, 2018: 103-106.\n",
    "\n",
    "[3] Aurelien Geron, 王静源, 贾玮, 边蕤, 邱俊涛. 机器学习实战：基于 Scikit-Learn 和 TensorFlow[M]. 北京: 机械工业出版社, 2018: 106-107."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
