{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-线性回归\" data-toc-modified-id=\"1.-线性回归-1\">1. 线性回归</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-基本形式\" data-toc-modified-id=\"1.1-基本形式-1.1\">1.1 基本形式</a></span></li><li><span><a href=\"#1.2-最小二乘法推导\" data-toc-modified-id=\"1.2-最小二乘法推导-1.2\">1.2 最小二乘法推导</a></span></li></ul></li><li><span><a href=\"#2.-Sklearn-实现\" data-toc-modified-id=\"2.-Sklearn-实现-2\">2. Sklearn 实现</a></span></li><li><span><a href=\"#参考资料\" data-toc-modified-id=\"参考资料-3\">参考资料</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相关文章：[机器学习 | 回归评估指标](https://blog.csdn.net/weixin_45488228/article/details/98897061)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 线性回归\n",
    "\n",
    "`线性回归`，又称**普通最小二乘法**（Ordinary Least Squares, OLS），是回归问题最简单也最经典的线性方法。线性回归需按照参数 w 和 b，使得对训练集的预测值与真实的回归目标值 y 之间的**均方误差**（MSE）最小。\n",
    "\n",
    "均方误差（[Mean Squared Error](https://blog.csdn.net/weixin_45488228/article/details/98897061)）是预测值与真实值之差的平方和除以样本数。\n",
    "\n",
    "线性回归没有参数，这是一个有点，但也因此无法控制模型的复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 基本形式 \n",
    "\n",
    "**线性回归预测模型：**\n",
    "\n",
    "$$f(x)=w_1 x_1 + w_2 x_2 + \\cdot \\cdot \\cdot + w_n x_n + b \\tag{1}$$\n",
    "\n",
    "- $f(x)$ 是预测值\n",
    "\n",
    "- $n$ 是特征的数量\n",
    "\n",
    "- $x_i$ 是第 $i$ 个特征值\n",
    "\n",
    "- 偏置项 $b$ 以及特征权重 $w_1, w_2,\\cdot \\cdot \\cdot ,w_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这可以用更为简介的向量化表示。\n",
    "\n",
    "**线性回归预测模型（向量化）：**\n",
    "\n",
    "$$\n",
    " \\begin{align*}  \n",
    "   f(x) &= w^T \\cdot x + b \\tag{2} \\\\  \n",
    "  &= \\hat{w}^T \\cdot x \\tag{3}\\\\   \n",
    " \\end{align*}\n",
    "$$\n",
    "\n",
    "-  $w=(w_1;w_2;...;w_n)$\n",
    "\n",
    "- $w$ 和 $b$ 学习得到，模型就得以确定\n",
    "\n",
    "- $\\hat{w}=(w;b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用`最小二乘法`最小化成本函数 ，可以得出 $\\hat{w}$ 的计算方程（标准方程）：<sup>[1]\n",
    "\n",
    "$$\\hat{w}^* = (X^T \\cdot X)^{(-1)} \\cdot X^T \\cdot y \\tag{4}$$\n",
    "\n",
    "- $\\hat{w}^*$ 是使成本函数 MSE 最小化的 $\\hat{w}$ 值\n",
    "\n",
    "- $y$ 是包含 $y^{(1)}$ 到 $y^{(m)}$ 的目标值向量\n",
    "\n",
    "则最终学得的`多元线性回归模型`为：\n",
    "\n",
    "$$f(\\hat{x}_i)=\\hat{x}_i(X^TX)^{-1}X^Ty \\tag{5}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 最小二乘法推导\n",
    "\n",
    "线性回归试图学得：\n",
    "\n",
    "$$f(x_i)=wx_i+b, 使得 f(x_i)\\simeq y_i \\tag{6}$$\n",
    "\n",
    "\n",
    "如何确定 $w$ 和 $b$ 呢？关键在于如何衡量 $f(x)$ 与 $y$ 之间的差别。\n",
    "\n",
    "回想一下，训练模型就是设置模型参数知道模型最适应训练集的过程。要达到这个目的，我们首先需要知道怎么衡量模型对训练数据的拟合程度是好还是差，在 [机器学习 | 回归评估指标](https://blog.csdn.net/weixin_45488228/article/details/98897061) 里，我们了解到回归模型最常见的性能指标有均方误差（MSE）。因此以 MSE 为线性回归模型的成本函数，在训练线性回归模型时，我们需要找到最小化 MSE 的 $w$ 值 $w^*$，即：\n",
    "\n",
    "$$\n",
    " \\begin{aligned}  \n",
    "   (w^*,b^*) &= \\mathop{\\arg\\min}\\limits_{(w,b)}\\sum_{i=1}^m(f(x_i)-y_i)^2 \\\\  \n",
    "  &= \\mathop{\\arg\\min}\\limits_{(w,b)}\\sum_{i=1}^m(y_i-wx_i-b)^2  \\\\    \n",
    " \\end{aligned} \\tag{7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "均方误差有非常好的几何意义，它对应了常用的欧几里得距离（或简称欧氏距离，Euclidean distance），基于均方误差最小化来进行模型求解的方法称为`“最小二乘法”`（least square method），在线性回归中，最小二乘法就是试图找出一条直线，使得所有样本到线上的欧氏距离之和最小。\n",
    "\n",
    "求解 $w$ 和 $b$ 使得 $E_{(w,b)}=\\sum_{i=1}^m(y_i-wx_i-b)^2$ 最小化的过程，称为`线性回归模型的最小二乘“参数估计”`（parameter estimation），我们可以将$E_{(w,b)}$ 分别对 $w$ 和 $b$ 求偏导，得到：<sup>[2]</sup>\n",
    "\n",
    "$$\n",
    " \\begin{align*}  \n",
    "   \\frac{\\partial E_{(w,b)}}{\\partial w} &= 2\\bigg(w\\sum_{i=1}^m x_i^2 - \\sum_{i=1}^m(y_i-b)x_i \\bigg)  \\tag{8} \\\\  \n",
    "  \\frac{\\partial E_{(w,b)}}{\\partial b} &= 2\\bigg(mb - \\sum_{i=1}^m(y_i-wx_i)\\bigg) \\tag{9}\\\\   \n",
    " \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后令公式 (8)、(9) 为零可以得到 $w$ 和 $b$ 最优解的闭式（closed-form）解：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "w & = \\frac{\\sum_{i=1}^my_i(x_i-\\bar{x})}{\\sum_{i=1}^mx_i^2 - \\frac{1}{m}\\big(\\sum_{i=1}^m x^i\\big)^2} \\tag{10} \\\\\n",
    "b & = \\frac{1}{m}\\sum_{i=1}^m(y_i-wx_i) \\tag{11} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "更一般的情形是对如有 $n$ 个属性的数据集 $D$ ，这时我们试图学得：\n",
    "\n",
    "$$f(x_i)=w^Tx_i+b, 使得 f(x_i)\\simeq y_i \\tag{12}$$\n",
    "\n",
    "这称为`多元线性回归`（multivariate linear regression）.\n",
    "\n",
    "类似的，可利用最小二乘法对 $w$ 和 $b$ 进行估计。为便于讨论，我们吧 $w$ 和 $b$ 转换为向量形式 $\\hat{w} = (w;b)$，相应的，吧数据集 $D$ 表示为一个 $m \\times (d+1)$ 大小的矩阵 $X$，其中每行对应与一个示例，该行前 $d$ 哥元素对应与示例的 $d$ 个属性值，最后一个元素恒为 1，即：\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\left( \\begin{array}{cc} \n",
    "x_{11} & x_{12} & \\cdots& x_{1n} & 1\\\\\n",
    "x_{21} & x_{22} & \\cdots\\ & x_{2n} & 1\\\\\n",
    "\\vdots & \\vdots & \\ddots\\ & \\vdots & \\vdots\\\\\n",
    "x_{m1} & x_{m2} & \\cdots& x_{mn} & 1\\\\\n",
    "\\end{array} \\right)\n",
    "=\n",
    "\\left( \\begin{array}{cc} \n",
    "x_{1n}^T & 1\\\\\n",
    "x_{2n}^T & 1\\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "x_{m}^T & 1\\\\\n",
    "\\end{array} \\right)\\tag{13}\n",
    "$$\n",
    "\n",
    "再把标记也写成向量形式 $y=(y_1;y_2;\\cdots;y_m)$，类似于公式 (7) ，有：\n",
    "\n",
    "$$w^* = \\mathop{\\arg\\min}\\limits_{\\hat{w}}(y-X\\hat{w})^T(y-X\\hat{w})\\tag{14}$$\n",
    "\n",
    "令 $E_{\\hat{w}}=(y-X\\hat{w})^T(y-X\\hat{w})$ ，对 $\\hat{w}$ 求导得到：\n",
    "\n",
    "$$\\frac{dE_{\\hat{w}}}{d\\hat{w}}=2X^T(X\\hat{w}-y)\\tag{15}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令上式为零可得 $\\hat{w}$ 的最优解的闭式接，但由于涉及矩阵逆的计算，比单变量情形要复杂一些，下面我们做一个简单的讨论。\n",
    "\n",
    "当 $X^TX$ 为满秩矩阵（full-rank matrix）或正定矩阵（positive definite matrix）时，令公式 (15) 为零可得`标准方程`：\n",
    "\n",
    "$$\\hat{w}^*=(X^TX)^{-1}X^Ty \\tag{16}$$\n",
    "\n",
    "其中 $(X^TX)^{-1}$ 是矩阵 $(X^TX)$ 的逆矩阵，令 $\\hat{x}_i=(x_i,1)$ ，则最终学得的`多元线性回归模型`为：\n",
    "\n",
    "$$f(\\hat{x}_i)=\\hat{x}_i(X^TX)^{-1}X^Ty \\tag{17}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sklearn 实现\n",
    "\n",
    "我们将使用线性回归根据体质指数 (BMI) 预测预期寿命。\n",
    "\n",
    "对于线性模型，我们将使用 sklearn.linear_model.LogisticRegression 类（[Sklearn 官方文档](https://scikit-learn.org/dev/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)）。\n",
    "\n",
    "我们将使用线性回归模型对数据进行拟合并预测 BMI 为 21.07931，所对应的预期寿命。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[60.31564716]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "bmi_life_data = pd.read_csv(\"data/bmi_and_life_expectancy.csv\")\n",
    "\n",
    "bmi_life_model = LinearRegression()\n",
    "bmi_life_model.fit(bmi_life_data[['BMI']], bmi_life_data[['Life expectancy']])\n",
    "\n",
    "laos_life_exp = bmi_life_model.predict(np.array(21.07931).reshape(-1,1))\n",
    "laos_life_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考资料\n",
    "\n",
    "[1] 周志华. 机器学习[M]. 北京: 清华大学出版社, 2016: 53-56\n",
    "\n",
    "[2] Aurelien Geron, 王静源, 贾玮, 边蕤, 邱俊涛. 机器学习实战：基于 Scikit-Learn 和 TensorFlow[M]. 北京: 机械工业出版社, 2018: 103-106.\n",
    "\n",
    "[3] Aurelien Geron, 王静源, 贾玮, 边蕤, 邱俊涛. 机器学习实战：基于 Scikit-Learn 和 TensorFlow[M]. 北京: 机械工业出版社, 2018: 106-107."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
